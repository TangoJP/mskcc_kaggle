{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing n-class word extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#sec1'><b>1. Extraction by frequency differences between classes</b></a>\n",
    "   - Thresholds for minimum fraction of documents appearing or per-doc frequency\n",
    "   - Threshold with absolute difference (by subtraction) or relative/fold difference (by division)\n",
    "\n",
    "<a href='#sec2'><b>2. Extraction by exclusive appearances in certain classes</b></a>\n",
    "   - Threshold for minimum fraction of documents appearing or per-doc frequency\n",
    "   \n",
    "<a href='#sec3'><b>3. Classification by Random Forest Classifier</b></a>\n",
    "   1. <a href='#sec3_1'>RFC with exclusive words</a>\n",
    "   2. <a href='#sec3_2'>RFC with words selected by relative difference - fraction of docs appearing</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib64/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import importlib as imp\n",
    "import feature_engineering.frequency_selection as fefs\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from pprint import pprint\n",
    "from matplotlib  import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import Counter\n",
    "#from gensim import corpora, matutils, models, similarities\n",
    "#from nltk import PorterStemmer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import and pre-process data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<ipython-input-2-45d7f3e0e033>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "  text_train = pd.read_csv(\"./data/training_text\", sep=\"\\|\\|\", engine='python',\n"
     ]
    }
   ],
   "source": [
    "class_train = pd.read_csv('./data/training_variants')\n",
    "text_train = pd.read_csv(\"./data/training_text\", sep=\"\\|\\|\", engine='python',\n",
    "                         header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "train = class_train.merge(text_train, on='ID')\n",
    "\n",
    "# create class label container\n",
    "class_labels = []\n",
    "for i in range(9):\n",
    "    class_labels.append('class' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/classified_docs.json') as f1:\n",
    "    classified_docs = json.load(f1)\n",
    "\n",
    "with open('./data/classified_tokenized_docs.json') as f2:\n",
    "    classified_tokenized_docs = json.load(f2)\n",
    "\n",
    "with open('./data/classified_texts.json') as f3:\n",
    "    classified_texts = json.load(f3)\n",
    "\n",
    "with open('./data/classified_tokenized_texts.json') as f4:\n",
    "    classified_tokenized_texts = json.load(f4)\n",
    "\n",
    "with open('./data/average_per_document_appearances.json') as f5:\n",
    "    ave_perdoc_apps = json.load(f5)\n",
    "    \n",
    "with open('./data/fraction_of_documents_with_appearance.json') as f6:\n",
    "    app_freqs = json.load(f6)\n",
    "\n",
    "with open(\"./data/unclassified_tokenized_docs.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    unclassified_tokenized_docs = list(reader)\n",
    "\n",
    "perdoc_apps = pd.DataFrame(ave_perdoc_apps).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# intersecting words among top%d appearing words in each class:  1391\n",
      "# intersecting words with >50% appearance:  287\n",
      "Table shape before removal:  (125448, 9)\n",
      "Table shape after removal:   (125161, 9)\n",
      "CPU times: user 359 ms, sys: 9.86 ms, total: 369 ms\n",
      "Wall time: 368 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fracdocs = pd.DataFrame(app_freqs).fillna(value=0)\n",
    "n = 3000\n",
    "\n",
    "top_words = []\n",
    "for i in range(9):\n",
    "    tops = fracdocs[class_labels[i]].sort_values(ascending=False).head(n)\n",
    "    top_words.append(list(tops.index))\n",
    "\n",
    "overlap1 = set(top_words[0])\n",
    "for lis in top_words[1:]:\n",
    "    overlap1.intersection_update(lis)\n",
    "print('# intersecting words among top%d appearing words in each class: ', len(overlap1))\n",
    "    \n",
    "remove_list = []\n",
    "for i in range(9):\n",
    "    remove_words = [word for word in overlap1 \\\n",
    "                    if word in fracdocs[class_labels[i]] \\\n",
    "                    if fracdocs[class_labels[i]][word] > 0.5]\n",
    "    remove_list.append(list(remove_words))\n",
    "\n",
    "overlap2 = set(remove_list[0])\n",
    "for lis in remove_list[1:]:\n",
    "    overlap2.intersection_update(lis)\n",
    "print('# intersecting words with >50% appearance: ', len(overlap2))\n",
    "\n",
    "fracdocs_update1 = fracdocs.copy()\n",
    "fracdocs_update1 = fracdocs_update1.drop(overlap2)\n",
    "print('Table shape before removal: ', fracdocs.shape)\n",
    "print('Table shape after removal:  ', fracdocs_update1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "<b>Extraction by fold difference  in frequency between classes</b>\n",
    "(<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== n-class words extractions by relative differecne ======\n",
      "Input Type: fraction_of_docs\n",
      "Minimum Difference = 1.40\n",
      "Minimum Frequency = 0.35\n",
      "# of words in one_class_words: 312\n",
      "# of words in two_class_words: 117\n",
      "# of words in three_class_words: 11\n",
      "# of words in four_class_words: 14\n",
      "# of words in five_class_words: 11\n",
      "# of words in six_class_words: 6\n",
      "# of words in seven_class_words: 25\n",
      "# of words in eight_class_words: 69\n",
      "# of words in other_words: 124581\n",
      "Total # of words: 125146\n",
      "\n",
      "\n",
      "====== n-class words extractions by absolute differecne ======\n",
      "Input Type: fraction_of_docs\n",
      "Minimum Difference = 0.20\n",
      "Minimum Frequency = 0.35\n",
      "# of words in one_class_words: 186\n",
      "# of words in two_class_words: 56\n",
      "# of words in three_class_words: 2\n",
      "# of words in four_class_words: 2\n",
      "# of words in five_class_words: 1\n",
      "# of words in six_class_words: 1\n",
      "# of words in seven_class_words: 2\n",
      "# of words in eight_class_words: 16\n",
      "# of words in other_words: 124885\n",
      "Total # of words: 125151\n",
      "\n",
      "\n",
      "====== n-class words extractions by relative differecne ======\n",
      "Input Type: per_doc_frequency\n",
      "Minimum Difference = 5.00\n",
      "Minimum Frequency = 2.00\n",
      "# of words in one_class_words: 102\n",
      "# of words in two_class_words: 39\n",
      "# of words in three_class_words: 2\n",
      "# of words in four_class_words: 3\n",
      "# of words in five_class_words: 5\n",
      "# of words in six_class_words: 1\n",
      "# of words in seven_class_words: 0\n",
      "# of words in eight_class_words: 1\n",
      "# of words in other_words: 125285\n",
      "Total # of words: 125438\n",
      "\n",
      "\n",
      "====== n-class words extractions by absolute differecne ======\n",
      "Input Type: per_doc_frequency\n",
      "Minimum Difference = 3.00\n",
      "Minimum Frequency = 2.00\n",
      "# of words in one_class_words: 246\n",
      "# of words in two_class_words: 54\n",
      "# of words in three_class_words: 6\n",
      "# of words in four_class_words: 1\n",
      "# of words in five_class_words: 1\n",
      "# of words in six_class_words: 2\n",
      "# of words in seven_class_words: 1\n",
      "# of words in eight_class_words: 3\n",
      "# of words in other_words: 125120\n",
      "Total # of words: 125434\n",
      "CPU times: user 35.5 s, sys: 166 ms, total: 35.6 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_class_words = fefs.getNClassWords(fracdocs_update1, doc_type='fraction_of_docs', mode='relative',\n",
    "                                min_frequency=0.35, min_difference=1.4, print_result=True)\n",
    "print('\\n')\n",
    "n_class_words = fefs.getNClassWords(fracdocs_update1, doc_type='fraction_of_docs', mode='absolute',\n",
    "                                min_frequency=0.35, min_difference=0.2, print_result=True)\n",
    "print('\\n')\n",
    "n_class_words = fefs.getNClassWords(perdoc_apps, doc_type='per_doc_frequency', mode='relative',\n",
    "                                min_frequency=2, min_difference=5, print_result=True)\n",
    "print('\\n')\n",
    "n_class_words = fefs.getNClassWords(perdoc_apps, doc_type='per_doc_frequency', mode='absolute',\n",
    "                                min_frequency=2, min_difference=3, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "<b>Extraction by exclusive appearance in certain classes</b>\n",
    "(<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== n-class words extractions by exclusive appearances =====\n",
      "Minimum Frequency = 0.150000\n",
      "# of words in one_class_words: 264\n",
      "# of words in two_class_words: 4\n",
      "# of words in three_class_words: 0\n",
      "# of words in four_class_words: 0\n",
      "# of words in five_class_words: 0\n",
      "# of words in six_class_words: 0\n",
      "# of words in seven_class_words: 0\n",
      "# of words in eight_class_words: 3\n",
      "# of words in other_words: 124890\n",
      "Total # of words: 125161\n",
      "CPU times: user 9.08 s, sys: 41 ms, total: 9.12 s\n",
      "Wall time: 9.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_freq = 0.15\n",
    "n_class_words_excl = fefs.getNClassExclusiveWords(fracdocs_update1, min_frequency=min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='sec3'></a>\n",
    "# 3. Classification with Random Forest Classifier\n",
    "(<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "select_words = fefs.selectNClassWords(n_class_words_excl, 2)\n",
    "print(len(select_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321, 268)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_result = fefs.myVectorizer(unclassified_tokenized_docs, select_words, type='count')\n",
    "vec_result['matrix'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting words...\n",
      "440 words extracted...\n",
      "Vectorizating texts...\n",
      "Training the classifier...\n",
      "Making predictions...\n",
      "===== Prediction Result =====\n",
      " - Accuracyl: 0.623\n",
      " - Log Loss: 1.925\n",
      "CPU times: user 14.2 s, sys: 91.3 ms, total: 14.3 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(class_train.Class).astype(int).ravel()\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50,\n",
    "                             n_jobs=7, random_state=33)\n",
    "\n",
    "fefs.RFC_NClassWords(unclassified_tokenized_docs, fracdocs_update1, y, \n",
    "                    n_class=3, doc_type='fraction_of_docs', extract_mode='relative', \n",
    "                    min_difference=1.4, min_frequency=0.35, vector_type='tfidf',\n",
    "                    test_size=0.15, random_state=None,\n",
    "                    rfc=rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting words...\n",
      "268 words extracted...\n",
      "Vectorizating texts...\n",
      "Training the classifier...\n",
      "Making predictions...\n",
      "===== Prediction Result =====\n",
      " - Accuracyl: 0.309\n",
      " - Log Loss: 1.785\n",
      "CPU times: user 13.3 s, sys: 78.4 ms, total: 13.4 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(class_train.Class).astype(int).ravel()\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50,\n",
    "                             n_jobs=7, random_state=33)\n",
    "\n",
    "fefs.RFC_NClassExclusiveWords(unclassified_tokenized_docs, fracdocs_update1, y, \n",
    "                    n_class=3, min_frequency=0.15, vector_type='tfidf',\n",
    "                    test_size=0.15, random_state=None,\n",
    "                    rfc=rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA & LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ncw_labels = ['one_class_words', 'two_class_words', 'three_class_words', \n",
    "                  'four_class_words', 'five_class_words', 'six_class_words', \n",
    "                  'seven_class_words', 'eight_class_words','other_words']\n",
    "\n",
    "n = 2\n",
    "freq = 0.35\n",
    "diff = 1.4\n",
    "min_freq = 0.15\n",
    "\n",
    "nclass_words = get_nClassWords(fracdocs_update1, doc_type='fraction_of_docs', mode='relative',\n",
    "                               min_frequency=freq, min_difference=diff, print_result=True)\n",
    "nclass_words_excl = get_nClassExclusiveWords(fracdocs_update1, min_frequency=min_freq)\n",
    "\n",
    "select_words = []\n",
    "for i in range(n):\n",
    "    select_words += nclass_words[ncw_labels[i]]\n",
    "print('%d words extracted by fold difference' % len(select_words))\n",
    "\n",
    "excl_words = nclass_words_excl[ncw_labels[0]] + nclass_words_excl[ncw_labels[1]]\n",
    "print('%d exclusive words extracted' % len(excl_words))\n",
    "select_words = select_words + excl_words\n",
    "\n",
    "select_words = list(set(select_words))\n",
    "print('%d unique words extracted in total' % len(select_words))\n",
    "\n",
    "# pre-process the raw texts\n",
    "print('Pre-processing texts...')\n",
    "unclf_docs2 = []\n",
    "for j, doc in enumerate(unclassified_tokenized_docs):\n",
    "    doc = [word for word in doc if word in select_words]\n",
    "    text = ' '.join(doc)\n",
    "    unclf_docs2.append(text)\n",
    "\n",
    "# Vectroize by Frequency Counts\n",
    "print('Vectorizating texts...')\n",
    "vectorizer = TfidfVectorizer()\n",
    "#vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(unclf_docs2).toarray()\n",
    "X = X.astype(float)\n",
    "y = np.array(class_train.Class).astype(int).ravel()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table with ID as index\n",
    "pca_table1 = pd.DataFrame(index=class_train.ID)\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "pca_table1['x'] = pca.transform(X).T[0]\n",
    "pca_table1['y'] = pca.transform(X).T[1]\n",
    "pca_table1['z'] = pca.transform(X).T[2]\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, elev=18, azim=55)#, rect=[0, 0, .95, 1])\n",
    "ax.scatter(pca_table1.x, pca_table1.y, pca_table1.z, c=class_train['Class'], marker = 'o', s=30, cmap = cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,5))\n",
    "ax1.scatter(pca_table1.x, pca_table1.y, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA1-PCA2')\n",
    "ax2.scatter(pca_table1.y, pca_table1.z, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA2-PCA3')\n",
    "ax3.scatter(pca_table1.z, pca_table1.x, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA3-PCA1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table with ID as index\n",
    "lda_table1 = pd.DataFrame(index=class_train.ID)\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "lda.fit(X, y)\n",
    "lda_table1['x'] = lda.transform(X).T[0]\n",
    "lda_table1['y'] = lda.transform(X).T[1]\n",
    "lda_table1['z'] = lda.transform(X).T[2]\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, elev=18, azim=55)#, rect=[0, 0, .95, 1])\n",
    "ax.scatter(lda_table1.x, lda_table1.y, lda_table1.z, c=class_train['Class'], marker = 'o', s=30, cmap = cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,5))\n",
    "ax1.scatter(lda_table1.x, lda_table1.y, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA1-PCA2')\n",
    "ax2.scatter(lda_table1.y, lda_table1.z, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA2-PCA3')\n",
    "ax3.scatter(lda_table1.z, lda_table1.x, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA3-PCA1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=7, random_state=33)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "pca.fit(X_train, y_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_train.shape, X_train_pca.shape)\n",
    "print(X_test.shape, X_test_pca.shape)\n",
    "\n",
    "rfc.fit(X_train_pca, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, rfc.predict(X_test_pca))\n",
    "lloss = log_loss(y_test, rfc.predict_proba(X_test_pca), labels=list(range(1, 10)))\n",
    "\n",
    "print('Accuracy %.3f' % accuracy)\n",
    "print('Log Loss %.3f' % lloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=7, random_state=33)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "lda.fit(X_train, y_train)\n",
    "X_train_lda = lda.transform(X_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "print(X_train.shape, X_train_lda.shape)\n",
    "print(X_test.shape, X_test_lda.shape)\n",
    "\n",
    "rfc.fit(X_train_lda, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, rfc.predict(X_test_lda))\n",
    "lloss = log_loss(y_test, rfc.predict_proba(X_test_lda), labels=list(range(1, 10)))\n",
    "\n",
    "print('Accuracy %.3f' % accuracy)\n",
    "print('Log Loss %.3f' % lloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Count vectorizer, also with 2-class exclusive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ncw_labels = ['one_class_words', 'two_class_words', 'three_class_words', \n",
    "                  'four_class_words', 'five_class_words', 'six_class_words', \n",
    "                  'seven_class_words', 'eight_class_words','other_words']\n",
    "\n",
    "n = 2\n",
    "freq = 0.35\n",
    "diff = 1.4\n",
    "min_freq = 0.15\n",
    "\n",
    "nclass_words = get_nClassWords(fracdocs_update1, doc_type='fraction_of_docs', mode='relative',\n",
    "                               min_frequency=freq, min_difference=diff, print_result=True)\n",
    "nclass_words_excl = get_nClassExclusiveWords(fracdocs_update1, min_frequency=min_freq)\n",
    "\n",
    "select_words = []\n",
    "for i in range(n):\n",
    "    select_words += nclass_words[ncw_labels[i]]\n",
    "print('%d words extracted by fold difference' % len(select_words))\n",
    "\n",
    "excl_words = nclass_words_excl[ncw_labels[0]] + nclass_words_excl[ncw_labels[1]]\n",
    "print('%d exclusive words extracted' % len(excl_words))\n",
    "select_words = select_words + excl_words\n",
    "\n",
    "select_words = list(set(select_words))\n",
    "print('%d unique words extracted in total' % len(select_words))\n",
    "\n",
    "# pre-process the raw texts\n",
    "print('Pre-processing texts...')\n",
    "unclf_docs2 = []\n",
    "for j, doc in enumerate(unclassified_tokenized_docs):\n",
    "    doc = [word for word in doc if word in select_words]\n",
    "    text = ' '.join(doc)\n",
    "    unclf_docs2.append(text)\n",
    "\n",
    "# Vectroize by Frequency Counts\n",
    "print('Vectorizating texts...')\n",
    "#vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(unclf_docs2).toarray()\n",
    "X = X.astype(float)\n",
    "y = np.array(class_train.Class).astype(int).ravel()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table with ID as index\n",
    "pca_table1 = pd.DataFrame(index=class_train.ID)\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "pca_table1['x'] = pca.transform(X).T[0]\n",
    "pca_table1['y'] = pca.transform(X).T[1]\n",
    "pca_table1['z'] = pca.transform(X).T[2]\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, elev=18, azim=55)#, rect=[0, 0, .95, 1])\n",
    "ax.scatter(pca_table1.x, pca_table1.y, pca_table1.z, c=class_train['Class'], marker = 'o', s=30, cmap = cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,5))\n",
    "ax1.scatter(pca_table1.x, pca_table1.y, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA1-PCA2')\n",
    "ax2.scatter(pca_table1.y, pca_table1.z, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA2-PCA3')\n",
    "ax3.scatter(pca_table1.z, pca_table1.x, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA3-PCA1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table with ID as index\n",
    "lda_table1 = pd.DataFrame(index=class_train.ID)\n",
    "lda = LinearDiscriminantAnalysis(n_components=3, solver='svd')\n",
    "lda.fit(X, y)\n",
    "lda_table1['x'] = lda.transform(X).T[0]\n",
    "lda_table1['y'] = lda.transform(X).T[1]\n",
    "lda_table1['z'] = lda.transform(X).T[2]\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, elev=18, azim=55)#, rect=[0, 0, .95, 1])\n",
    "ax.scatter(lda_table1.x, lda_table1.y, lda_table1.z, c=class_train['Class'], marker = 'o', s=30, cmap = cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,5))\n",
    "ax1.scatter(lda_table1.x, lda_table1.y, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA1-PCA2')\n",
    "ax2.scatter(lda_table1.y, lda_table1.z, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA2-PCA3')\n",
    "ax3.scatter(lda_table1.z, lda_table1.x, c=class_train['Class'], marker = 'o', s=50, cmap = cm.jet, label='PCA3-PCA1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=7, random_state=33)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "pca.fit(X_train, y_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_train.shape, X_train_pca.shape)\n",
    "print(X_test.shape, X_test_pca.shape)\n",
    "\n",
    "rfc.fit(X_train_pca, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, rfc.predict(X_test_pca))\n",
    "lloss = log_loss(y_test, rfc.predict_proba(X_test_pca), labels=list(range(1, 10)))\n",
    "\n",
    "print('Accuracy %.3f' % accuracy)\n",
    "print('Log Loss %.3f' % lloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=7, random_state=33)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "lda.fit(X_train, y_train)\n",
    "X_train_lda = lda.transform(X_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "print(X_train.shape, X_train_lda.shape)\n",
    "print(X_test.shape, X_test_lda.shape)\n",
    "\n",
    "rfc.fit(X_train_lda, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, rfc.predict(X_test_lda))\n",
    "lloss = log_loss(y_test, rfc.predict_proba(X_test_lda), labels=list(range(1, 10)))\n",
    "\n",
    "print('Accuracy %.3f' % accuracy)\n",
    "print('Log Loss %.3f' % lloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
