{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting 2-gram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted words here will be combined with others extracted in different ways\n",
    "\n",
    "<a href='#sec1'><b>1. Import and pre-process data</b></a>\n",
    "\n",
    "<a href='#sec2'><b>2. Create features based on frequency paramerization</b></a>\n",
    "   \n",
    "<a href='#sec3'><b>3. PCA & LDA</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as imp\n",
    "import feature_engineering.frequency_selection as fefs\n",
    "import feature_engineering.text_processing as fetp\n",
    "import myplot.decomposition as mpd\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# 1. Import and pre-process data\n",
    "(<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_train = pd.read_csv('./data/training_variants')\n",
    "text_train = pd.read_csv(\"./data/training_text\", sep=r\"\\|\\|\", engine='python',\n",
    "                         header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "train = class_train.merge(text_train, on='ID')\n",
    "\n",
    "# create class label container\n",
    "class_labels = []\n",
    "for i in range(9):\n",
    "    class_labels.append('class' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>1109</td>\n",
       "      <td>FANCA</td>\n",
       "      <td>S1088F</td>\n",
       "      <td>1</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1277</td>\n",
       "      <td>ARID5B</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1407</td>\n",
       "      <td>FGFR3</td>\n",
       "      <td>K508M</td>\n",
       "      <td>6</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>1639</td>\n",
       "      <td>FLT1</td>\n",
       "      <td>Amplification</td>\n",
       "      <td>6</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2755</td>\n",
       "      <td>BRAF</td>\n",
       "      <td>G596C</td>\n",
       "      <td>7</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    Gene             Variation  Class  Text\n",
       "1109  1109   FANCA                S1088F      1  null\n",
       "1277  1277  ARID5B  Truncating Mutations      1  null\n",
       "1407  1407   FGFR3                 K508M      6  null\n",
       "1639  1639    FLT1         Amplification      6  null\n",
       "2755  2755    BRAF                 G596C      7  null"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['Text'] == 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train before removing null entries: 3321\n",
      "Length of train before removing null entries: 3316\n"
     ]
    }
   ],
   "source": [
    "print('Length of train before removing null entries: %d' % len(train))\n",
    "train = train.drop(train.index[train['Text'] == 'null'])\n",
    "print('Length of train before removing null entries: %d' % len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test on one piece of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process the entire text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whole_corpus = (pd.read_csv('./data/unclassified_stemmed_corpus.csv', header=None, squeeze=True)).tolist()\n",
    "filtered_2gram_words = (pd.read_csv('./data/filtered_2gram_words.csv', header=None, squeeze=True)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3316"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.8 s, sys: 3.72 s, total: 32.6 s\n",
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vectorizer with the filtered terms\n",
    "ngram_vectorizer2 = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), vocabulary=filtered_2gram_words)\n",
    "vectorized_doc = ngram_vectorizer2.fit_transform(whole_corpus)\n",
    "X = vectorized_doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ngram_vectorizer2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [None]*len(vocab)\n",
    "for key, val in vocab.items():\n",
    "    v[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix(X, vocab_list, num_split=5):\n",
    "    num_feat = X.shape[1]\n",
    "    if num_feat != len(vocab_list):\n",
    "        print('Error: # features and vocabulary does not match!')\n",
    "        return\n",
    "    \n",
    "    n = num_feat // num_split\n",
    "    indices = [0, n]\n",
    "    split_Xs = []\n",
    "    split_Vocabs = []\n",
    "    for i in range(num_split-1):\n",
    "        start = indices[i]\n",
    "        end = indices[i+1]\n",
    "        indices.append(end+n)\n",
    "        split_Xs.append(X[:, start:end])\n",
    "        split_Vocabs.append(vocab_list[start:end])\n",
    "    split_Xs.append(X[:, end:])\n",
    "    split_Vocabs.append(X[:, end:])\n",
    "                        \n",
    "    return split_Xs, split_Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_importance_features(rfc, X, y, vocab_list, percentile=0.95, test_size=0.15,\n",
    "                                     print_scores=True, random_state=None):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "                train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    if print_scores:\n",
    "        print('Accuracy %.3f' % accuracy_score(y_test, rfc.predict(X_test)))\n",
    "        print('Log Loss %.3f' % log_loss(y_test, rfc.predict_proba(X_test), \n",
    "                                         labels=list(range(1, 10))))\n",
    "    \n",
    "    imps = rfc.feature_importances_\n",
    "    selectF_indices = list(np.argwhere(imps >= np.percentile(imps, (100*percentile))).ravel())\n",
    "    selectX = X[:, selectF_indices]\n",
    "    selectVocab = [vocab_list[i] for i,_ in enumerate(selectF_indices)]\n",
    "    \n",
    "    return selectX, selectVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myRFE(rfc, X, y, vocab_list,\n",
    "          num_split=10, percentile=0.95, test_size=0.15, \n",
    "          print_scores=True, random_state=None):\n",
    "    \n",
    "    selection = []\n",
    "    selection_vocab = []\n",
    "    split_Xs, split_Vocabs = split_matrix(X, vocab_list, num_split=num_split)\n",
    "    for i in range(num_split):\n",
    "        print('>> Processing split%d' % (i+1))\n",
    "        selectX, selectVocab = remove_zero_importance_features(rfc, split_Xs[i], y, split_Vocabs[i],\n",
    "                                          percentile=percentile, test_size=test_size,\n",
    "                                          print_scores=print_scores, random_state=random_state)\n",
    "        selection.append(selectX)\n",
    "        selection_vocab.append(selectVocab)\n",
    "        \n",
    "    newX = np.concatenate(selection, axis=1)\n",
    "    newVocab = np.concatenate(selection_vocab, axis=1)\n",
    "    \n",
    "    return newX, newVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Processing split1\n",
      "Accuracy 0.649\n",
      "Log Loss 1.016\n",
      ">> Processing split2\n",
      "Accuracy 0.651\n",
      "Log Loss 1.067\n",
      ">> Processing split3\n",
      "Accuracy 0.653\n",
      "Log Loss 1.000\n",
      ">> Processing split4\n",
      "Accuracy 0.643\n",
      "Log Loss 1.006\n",
      ">> Processing split5\n",
      "Accuracy 0.651\n",
      "Log Loss 1.002\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3316 is out of bounds for axis 0 with size 3316",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-4f93bdc0961a>\u001b[0m in \u001b[0;36mmyRFE\u001b[0;34m(rfc, X, y, vocab_list, num_split, percentile, test_size, print_scores, random_state)\u001b[0m\n\u001b[1;32m     10\u001b[0m         selectX, selectVocab = remove_zero_importance_features(rfc, split_Xs[i], y, split_Vocabs[i],\n\u001b[1;32m     11\u001b[0m                                           \u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                           print_scores=print_scores, random_state=random_state)\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselectX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mselection_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselectVocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-3748c83d547a>\u001b[0m in \u001b[0;36mremove_zero_importance_features\u001b[0;34m(rfc, X, y, vocab_list, percentile, test_size, print_scores, random_state)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mselectF_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mselectX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectF_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mselectVocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselectF_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mselectX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-3748c83d547a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mselectF_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mselectX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectF_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mselectVocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselectF_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mselectX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3316 is out of bounds for axis 0 with size 3316"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(train['Class']).astype(int).ravel()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=8, random_state=33)\n",
    "newX, newVocab = myRFE(rfc, X, y, v,\n",
    "                          num_split=5, percentile=0.95, test_size=0.15, \n",
    "                          print_scores=True, random_state=345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3316, 51160)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.687\n",
      "Log Loss 1.008\n",
      "CPU times: user 1min 24s, sys: 899 ms, total: 1min 25s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(train['Class']).astype(int).ravel()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=8, random_state=33)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "                train_test_split(newX, y, test_size=0.15, random_state=345)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Accuracy %.3f' % accuracy_score(y_test, rfc.predict(X_test)))\n",
    "print('Log Loss %.3f' % log_loss(y_test, rfc.predict_proba(X_test), \n",
    "                                 labels=list(range(1, 10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.687\n",
      "Log Loss 1.008\n",
      "Accuracy 0.683\n",
      "Log Loss 1.061\n",
      "(3316, 10232)\n",
      "CPU times: user 1min 40s, sys: 1.21 s, total: 1min 42s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(train['Class']).astype(int).ravel()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=8, random_state=33)\n",
    "\n",
    "newX1 = remove_zero_importance_features(rfc, newX, y, percentile=0.8, test_size=0.15,\n",
    "                                          print_scores=True, random_state=345)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "                train_test_split(newX1, y, test_size=0.15, random_state=345)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Accuracy %.3f' % accuracy_score(y_test, rfc.predict(X_test)))\n",
    "print('Log Loss %.3f' % log_loss(y_test, rfc.predict_proba(X_test), \n",
    "                                 labels=list(range(1, 10))))\n",
    "print(newX1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 10232 features.\n",
      "Fitting estimator with 10232 features.\n",
      "Fitting estimator with 10232 features.\n",
      "Fitting estimator with 9209 features.\n",
      "Fitting estimator with 9209 features.\n",
      "Fitting estimator with 9209 features.\n",
      "Fitting estimator with 8186 features.\n",
      "Fitting estimator with 8186 features.\n",
      "Fitting estimator with 8186 features.\n",
      "Fitting estimator with 7163 features.\n",
      "Fitting estimator with 7163 features.\n",
      "Fitting estimator with 7163 features.\n",
      "Fitting estimator with 6140 features.\n",
      "Fitting estimator with 6140 features.\n",
      "Fitting estimator with 6140 features.\n",
      "Fitting estimator with 5117 features.\n",
      "Fitting estimator with 5117 features.\n",
      "Fitting estimator with 4094 features.\n",
      "Fitting estimator with 5117 features.\n",
      "Fitting estimator with 4094 features.\n",
      "Fitting estimator with 3071 features.\n",
      "Fitting estimator with 4094 features.\n",
      "Fitting estimator with 3071 features.\n",
      "Fitting estimator with 2048 features.\n",
      "Fitting estimator with 3071 features.\n",
      "Fitting estimator with 2048 features.\n",
      "Fitting estimator with 1025 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 2048 features.\n",
      "Fitting estimator with 1025 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 1025 features.\n",
      "Fitting estimator with 2 features.\n",
      "CPU times: user 2min 10s, sys: 1.19 s, total: 2min 11s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selector = RFECV(rfc, step=0.1, scoring='neg_log_loss', verbose=1, n_jobs=8)\n",
    "selector = selector.fit(newX1, y)\n",
    "rfe_indices = list(np.argwhere(selector.support_ == True).ravel())\n",
    "X_2grams = newX[:, rfe_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3316, 10232)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newX1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3316, 5117)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2grams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.675\n",
      "Log Loss 0.967\n",
      "CPU times: user 8.93 s, sys: 113 ms, total: 9.04 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(train['Class']).astype(int).ravel()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=8, random_state=33)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "                train_test_split(X_2grams, y, test_size=0.15, random_state=345)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Accuracy %.3f' % accuracy_score(y_test, rfc.predict(X_test)))\n",
    "print('Log Loss %.3f' % log_loss(y_test, rfc.predict_proba(X_test), \n",
    "                                 labels=list(range(1, 10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA & LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decomp_table = mpd.decomposition3D(selectX1, train['Class'])\n",
    "mpd.decomposition3DPlot(decomp_table, train['Class'])\n",
    "mpd.decomposition2DPlot(decomp_table, train['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "decomp_table = mpd.decomposition3D(selectX1, train['Class'], \n",
    "                y=y, decomposer=LinearDiscriminantAnalysis(n_components=3))\n",
    "mpd.decomposition3DPlot(decomp_table, train['Class'])\n",
    "mpd.decomposition2DPlot(decomp_table, train['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=10, n_components=3, n_jobs=8)\n",
    "decomp_table = mpd.decomposition3D(selectX1, train['Class'], \n",
    "                                y=y, decomposer=kpca)\n",
    "mpd.decomposition3DPlot(decomp_table, train['Class'])\n",
    "mpd.decomposition2DPlot(decomp_table, train['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
