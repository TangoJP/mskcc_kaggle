{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>\n",
    "# Basic Analysis of Word Frequencies\n",
    "- <a href='#sec1'>Create classified datasets</a>\n",
    "- <a href='#sec2'>Basic Frequency Analysis</a>\n",
    "- <a href='#sec3'></a>\n",
    "- <a href='#sec4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib64/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import mskcc_functions as ski\n",
    "import scipy.stats as scs\n",
    "import feature_engineering as fe\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from pprint import pprint\n",
    "from matplotlib  import cm\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "from gensim import corpora, matutils, models, similarities\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<input>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "<ipython-input-2-4467bb76c423>:2: DeprecationWarning: invalid escape sequence \\|\n",
      "  text_train = pd.read_csv(\"./data/training_text\", sep=\"\\|\\|\", engine='python',\n"
     ]
    }
   ],
   "source": [
    "class_train = pd.read_csv('./data/training_variants')\n",
    "text_train = pd.read_csv(\"./data/training_text\", sep=\"\\|\\|\", engine='python',\n",
    "                         header=None, skiprows=1, names=[\"ID\",\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = class_train.merge(text_train, on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# Create classified datasets (<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Work Flow</b>\n",
    "1. Create \"classified_docs to\" hold all the docs (str) in each class\n",
    "2. Convert each doc in each class to a list of words with tokenizer\n",
    "3. Remove special character-containing words and stop words from each word_list in each class\n",
    "4. Apply stemmer to each word in each word_list in each class\n",
    "\n",
    "\n",
    "- classified_docs : dictionary<br>\n",
    "    keys are class labels. values a lists, each of which contains strings from each 'Text' entry in train table\n",
    "- classified_tokenized_docs : dictionary<br>\n",
    "    keys are class labels. values are lists of lists. Inner list is a list of words from each 'Text' entry in train table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create class label container\n",
    "class_labels = []\n",
    "for i in range(9):\n",
    "    class_labels.append('class' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class1 being processed...\n",
      "class2 being processed...\n",
      "class3 being processed...\n",
      "class4 being processed...\n",
      "class5 being processed...\n",
      "class6 being processed...\n",
      "class7 being processed...\n",
      "class8 being processed...\n",
      "class9 being processed...\n",
      "CPU times: user 7min 45s, sys: 381 ms, total: 7min 45s\n",
      "Wall time: 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make a stemmer object, define stop words\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# create classified_docs\n",
    "classified_docs = {}\n",
    "classified_tokenized_docs = {}\n",
    "for i in range(9):\n",
    "    print('%s being processed...' % class_labels[i])\n",
    "    docs = [doc for j, doc in enumerate(train[train.Class == (i+1)]['Text'])]\n",
    "    classified_docs[class_labels[i]] = docs\n",
    "    \n",
    "    tokenized_docs = []\n",
    "    for k, doc in enumerate(docs):\n",
    "        # tokenize the doc (DO NOT MAKE IT A SET FOR LATER USE)\n",
    "        tokenized_doc = word_tokenize(fe.replace_with_whitespace(doc, hyphens='on'))\n",
    "\n",
    "        # Remove stop words and words with special characters\n",
    "        tokenized_doc = [word for word in tokenized_doc \\\n",
    "                         if re.search(r'^[A-Za-z]', word) \\\n",
    "                         if re.search(r'[A-Za-z0-9]$', word) \\\n",
    "                         if not re.search(r'[@#%&*()+=]', word) \\\n",
    "                         if len(word) > 1 \\\n",
    "                         if word.lower() not in stop_words]\n",
    "\n",
    "        # Apply stemmer to each word in the list\n",
    "        tokenized_doc = [stemmer.stem(word) for word in tokenized_doc]\n",
    "        \n",
    "        tokenized_docs.append(tokenized_doc)\n",
    "    \n",
    "    classified_tokenized_docs[class_labels[i]] = tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class1 check:  True\n",
      "class2 check:  True\n",
      "class3 check:  True\n",
      "class4 check:  True\n",
      "class5 check:  True\n",
      "class6 check:  True\n",
      "class7 check:  True\n",
      "class8 check:  True\n",
      "class9 check:  True\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    len_docs = len(classified_docs[class_labels[i]])\n",
    "    len_tokenized_docs = len(classified_tokenized_docs[class_labels[i]])\n",
    "    print('%s check: ' % class_labels[i], (len_docs == len_tokenized_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create combined big text for each class</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- classified_texts : dictionary<br>\n",
    "    keys are class labels. values are strings made by combining all the strings from 'Text' entries in each class in train table\n",
    "- classified_tokenized_texts : dictionary<br>\n",
    "    keys are class labels. values are lists of words in the combined strings from 'Text' entries in each class in train table\n",
    "The latter is processed the same way as non-combined dictionary above (i.e. classified_tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class1 being processed...\n",
      "class2 being processed...\n",
      "class3 being processed...\n",
      "class4 being processed...\n",
      "class5 being processed...\n",
      "class6 being processed...\n",
      "class7 being processed...\n",
      "class8 being processed...\n",
      "class9 being processed...\n",
      "CPU times: user 8min 6s, sys: 27.9 s, total: 8min 34s\n",
      "Wall time: 8min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classified_texts = {}\n",
    "classified_tokenized_texts = {}\n",
    "for i in range(9):\n",
    "    print('%s being processed...' % class_labels[i])\n",
    "    docs = [doc for doc in train[train.Class == (i+1)]['Text']]\n",
    "    text = ''\n",
    "    for doc in docs:\n",
    "        text = text + doc + ' '\n",
    "    \n",
    "    tokenized_text = word_tokenize(fe.replace_with_whitespace(text, hyphens='on'))\n",
    "    tokenized_text = [word for word in tokenized_text \\\n",
    "                         if re.search(r'^[A-Za-z]', word) \\\n",
    "                         if re.search(r'[A-Za-z0-9]$', word) \\\n",
    "                         if not re.search(r'[@#%&*()+=]', word) \\\n",
    "                         if len(word) > 1 \\\n",
    "                         if word.lower() not in stop_words]\n",
    "    tokenized_text = [stemmer.stem(word) for word in tokenized_text]\n",
    "    \n",
    "    classified_texts[class_labels[i]] = text\n",
    "    classified_tokenized_texts[class_labels[i]] = tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Save dictionaries for later use</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/classified_docs.json', 'w') as f1:\n",
    "    json.dump(classified_docs, f1)\n",
    "\n",
    "with open('./data/classified_tokenized_docs.json', 'w') as f2:\n",
    "    json.dump(classified_tokenized_docs, f2)\n",
    "\n",
    "with open('./data/classified_texts.json', 'w') as f3:\n",
    "    json.dump(classified_texts, f3)\n",
    "\n",
    "with open('./data/classified_tokenized_texts.json', 'w') as f4:\n",
    "    json.dump(classified_tokenized_texts, f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "# Basic Frequency Analysis  (<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>average per-doc appearance & appearance frequency (num_docs with the word / total_num_docs)</b>\n",
    "\n",
    "\\*Use dictionaries and pandas\n",
    "- Create a list of unique words for each class\n",
    "- Average per-doc appearances\n",
    "  Count number of appearances per document and average over number of documents in each class\n",
    "- Appearance frequency (num_docs with the word / total_num_docs)\n",
    "  Check if a word appearcs in a document or not, and calculate the fraction of documents that contain the word   for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique words in class1: 50671\n",
      "# of unique words in class2: 42200\n",
      "# of unique words in class3: 9331\n",
      "# of unique words in class4: 41992\n",
      "# of unique words in class5: 17022\n",
      "# of unique words in class6: 19101\n",
      "# of unique words in class7: 64771\n",
      "# of unique words in class8: 7881\n",
      "# of unique words in class9: 8949\n"
     ]
    }
   ],
   "source": [
    "classified_unique_sets = {}\n",
    "for i in range(9):\n",
    "    tokenized_text = classified_tokenized_texts[class_labels[i]]\n",
    "    unique_set = list(set(tokenized_text))\n",
    "    classified_unique_sets[class_labels[i]] = unique_set\n",
    "    print('# of unique words in %s: %d'% (class_labels[i], len(unique_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class1 being processed...\n",
      "class2 being processed...\n",
      "class3 being processed...\n",
      "class4 being processed...\n",
      "class5 being processed...\n",
      "class6 being processed...\n",
      "class7 being processed...\n",
      "class8 being processed...\n",
      "class9 being processed...\n",
      "CPU times: user 28.3 s, sys: 424 ms, total: 28.7 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ave_perdoc_apps = {}\n",
    "app_freqs = {}\n",
    "for i in range(9):\n",
    "    print('%s being processed...' % class_labels[i])\n",
    "    tokenized_docs = classified_tokenized_docs[class_labels[i]]\n",
    "    num_docs = len(tokenized_docs)\n",
    "    \n",
    "    tokenized_text = classified_tokenized_texts[class_labels[i]]\n",
    "    c = Counter(tokenized_text)\n",
    "    ave_perdoc_app = dict(c)\n",
    "    ave_perdoc_app = {key:(value/num_docs) for key, value in ave_perdoc_app.items()}\n",
    "    \n",
    "    app_freq_list = []\n",
    "    for doc in tokenized_docs:\n",
    "        c = Counter(doc)\n",
    "        freq = dict(c)\n",
    "        app_freq = {key:1 for key, value in freq.items() if value > 0}\n",
    "        app_freq_list.append(app_freq)\n",
    "    app_freq_table = pd.DataFrame(app_freq_list)\n",
    "    app_freq = dict(app_freq_table.sum(axis=0)/num_docs)\n",
    "    \n",
    "    ave_perdoc_apps[class_labels[i]] = ave_perdoc_app\n",
    "    app_freqs[class_labels[i]] = app_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/average_per_document_appearances.json', 'w') as f5:\n",
    "    json.dump(ave_perdoc_apps, f5)\n",
    "    \n",
    "with open('./data/fraction_of_documents_with_appearance.json', 'w') as f6:\n",
    "    json.dump(app_freqs, f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reload the dictionaries from json files</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/classified_docs.json') as f1:\n",
    "    classified_docs = json.load(f1)\n",
    "\n",
    "with open('./data/classified_tokenized_docs.json') as f2:\n",
    "    classified_tokenized_docs = json.load(f2)\n",
    "\n",
    "with open('./data/classified_texts.json') as f3:\n",
    "    classified_texts = json.load(f3)\n",
    "\n",
    "with open('./data/classified_tokenized_texts.json') as f4:\n",
    "    classified_tokenized_texts = json.load(f4)\n",
    "\n",
    "with open('./data/average_per_document_appearances.json') as f5:\n",
    "    ave_perdoc_apps = json.load(f5)\n",
    "    \n",
    "with open('./data/fraction_of_documents_with_appearance.json') as f6:\n",
    "    app_freqs = json.load(f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Remove frequent words that appear in >50% of docs in each class</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get top 2000 words in terms of fraction of documents that they appear within each class\n",
    "- Make a set for intersection of above words among all classes\n",
    "- Remove words in the set if they appear in more than 50% of the documents in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# intersecting words among top%d appearing words in each class:  1028\n",
      "# intersecting words with >50% appearance:  287\n",
      "Table shape before removal:  (125448, 9)\n",
      "Table shape after removal:   (125161, 9)\n"
     ]
    }
   ],
   "source": [
    "fracdocs = pd.DataFrame(app_freqs).fillna(value=0)\n",
    "n = 2000\n",
    "\n",
    "top_words = []\n",
    "for i in range(9):\n",
    "    tops = fracdocs[class_labels[i]].sort_values(ascending=False).head(n)\n",
    "    top_words.append(list(tops.index))\n",
    "\n",
    "overlap1 = set(top_words[0])\n",
    "for lis in top_words[1:]:\n",
    "    overlap1.intersection_update(lis)\n",
    "print('# intersecting words among top%d appearing words in each class: ', len(overlap1))\n",
    "    \n",
    "remove_list = []\n",
    "for i in range(9):\n",
    "    remove_words = [word for word in overlap1 \\\n",
    "                    if word in fracdocs[class_labels[i]] \\\n",
    "                    if fracdocs[class_labels[i]][word] > 0.5]\n",
    "    remove_list.append(list(remove_words))\n",
    "\n",
    "overlap2 = set(remove_list[0])\n",
    "for lis in remove_list[1:]:\n",
    "    overlap2.intersection_update(lis)\n",
    "print('# intersecting words with >50% appearance: ', len(overlap2))\n",
    "\n",
    "fracdocs_update1 = fracdocs.copy()\n",
    "fracdocs_update1 = fracdocs_update1.drop(overlap2)\n",
    "print('Table shape before removal: ', fracdocs.shape)\n",
    "print('Table shape after removal:  ', fracdocs_update1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1-class appearance words (8-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "one_class_words = []\n",
    "for i, word in enumerate(fracdocs.index):\n",
    "    apps = np.array(fracdocs.loc[word])\n",
    "    apps[::-1].sort()\n",
    "    if (apps[0] - apps[1]) >= threshold:\n",
    "        one_class_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# more general version of above\n",
    "# make n(_)c(lass_)w(ords)_labels\n",
    "ncw_labels = ['one_class_words', 'two_class_words', 'three_class_words', \n",
    "              'four_class_words', 'five_class_words', 'six_class_words', \n",
    "              'seven_class_words', 'eight_class_words','other_words']\n",
    "\n",
    "# Create a new dictionary to contain each n-class of words in list formats\n",
    "n_class_words = {}\n",
    "for i in range(9):\n",
    "    n_class_words[ncw_labels[i]] = []\n",
    "\n",
    "# Get words for each n-class of words (might be a better way to do this?)\n",
    "threshold = 0.5\n",
    "for j, word in enumerate(fracdocs.index):\n",
    "    apps = np.array(fracdocs.loc[word])\n",
    "    apps[::-1].sort()\n",
    "    if (apps[0] - apps[1]) >= threshold:\n",
    "        n_class_words[ncw_labels[0]].append(word)\n",
    "    elif (apps[1] - apps[2]) >= threshold:\n",
    "        n_class_wors[ncw_labels[1]].append(word)\n",
    "    elif (apps[2] - apps[3]) >= threshold:\n",
    "        n_class_wors[ncw_labels[2]].append(word)\n",
    "    elif (apps[3] - apps[4]) >= threshold:\n",
    "        n_class_wors[ncw_labels[3]].append(word)\n",
    "    elif (apps[4] - apps[5]) >= threshold:\n",
    "        n_class_wors[ncw_labels[4]].append(word)\n",
    "    elif (apps[5] - apps[6]) >= threshold:\n",
    "        n_class_wors[ncw_labels[5]].append(word)\n",
    "    elif (apps[6] - apps[7]) >= threshold:\n",
    "        n_class_wors[ncw_labels[6]].append(word)\n",
    "    elif (apps[7] - apps[8]) >= threshold:\n",
    "        n_class_wors[ncw_labels[7]].append(word)\n",
    "    else:\n",
    "        n_class_wors[ncw_labels[8]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    print('# of words in %s: %d' % (ncw_labels[i], len(n_class_wors[ncw_labels[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop version. Prob very slow\n",
    "threshold = 0.5\n",
    "for i in range(1):\n",
    "    # Pre-screen out any word with <0.5 appearance frequency\n",
    "    class_words = [word for word in fracdocs[class_labels[i]] if fracdocs[class_labels[i]][word] >= 0.5]\n",
    "    \n",
    "    other_classes = [class_label for class_label in class_labels if class_label != class_labels[i]]\n",
    "    fracdocs_other_classes = fracdocs[other_classes]\n",
    "    one_class_words = [word for word in class_words \\\n",
    "                       if (np.max(fracdocs_other_classes.loc[word]) - fracdocs[class_labels[i]][word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2-class appearance words (7-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3-class appearance words (6-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4-class appearance words (5-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5-class appearance words (4-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6-class appearance words (3-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7-class appearance words (2-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>8-class appearance words (1-class non-appearance)</b>\n",
    "\n",
    "Get words whose appearance frequency is more than 0.5 higher in one class than in all the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
