{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting 2-gram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted words here will be combined with others extracted in different ways\n",
    "\n",
    "<a href='#sec1'><b>1. Import and pre-process data</b></a>\n",
    "\n",
    "<a href='#sec2'><b>2. Create features based on frequency paramerization</b></a>\n",
    "   \n",
    "<a href='#sec3'><b>3. PCA & LDA</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as imp\n",
    "import feature_engineering.frequency_selection as fefs\n",
    "import feature_engineering.text_processing as fetp\n",
    "import myplot.decomposition as mpd\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# 1. Import and pre-process data\n",
    "(<a href='#sec0'>Back to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_train = pd.read_csv('./data/training_variants')\n",
    "text_train = pd.read_csv(\"./data/training_text\", sep=r\"\\|\\|\", engine='python',\n",
    "                         header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "train = class_train.merge(text_train, on='ID')\n",
    "\n",
    "# create class label container\n",
    "class_labels = []\n",
    "for i in range(9):\n",
    "    class_labels.append('class' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/classified_texts.json') as f3:\n",
    "    classified_texts = json.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The RNA maturation is an important and complex biological process. It requires several small nuclear ribonucleoproteins (snRNPs) that comprise the two forms of spliceosomes. The major form of spliceosome (U2-type) is composed of U1, U2, U4/6 and U5 snRNPs, and catalyzes most splicing events in metaz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_texts['class9'][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = classified_texts['class1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform([doc1])\n",
    "class_dict = {value:key for key, value in ngram_vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {value:key for key, value in ngram_vectorizer.vocabulary_.items()}\n",
    "table1 = pd.DataFrame(counts.toarray())\n",
    "table2 = table1.rename(columns=class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>796572</th>\n",
       "      <th>796573</th>\n",
       "      <th>796574</th>\n",
       "      <th>796575</th>\n",
       "      <th>796576</th>\n",
       "      <th>796577</th>\n",
       "      <th>796578</th>\n",
       "      <th>796579</th>\n",
       "      <th>796580</th>\n",
       "      <th>796581</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 796582 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0       8       2       4       2       2       2       1       5      24   \n",
       "\n",
       "   9        ...    796572  796573  796574  796575  796576  796577  796578  \\\n",
       "0      26   ...         2       9       9       1       1       1       3   \n",
       "\n",
       "   796579  796580  796581  \n",
       "0       3       3       1  \n",
       "\n",
       "[1 rows x 796582 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unfiltered_2gram_words = []\n",
    "for doc in train.iloc[:5]['Text']:\n",
    "    ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    ngram_vectorizer.fit([doc])\n",
    "    unfiltered_2gram_words += ngram_vectorizer.get_feature_names()\n",
    "unfiltered_2gram_words = list(set(unfiltered_2gram_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter: 14366 terms\n",
      "After filter: 5652 terms\n",
      "CPU times: user 102 ms, sys: 0 ns, total: 102 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filtered_2gram_words = []\n",
    "filtered_2gram_words_indices = []\n",
    "for i, term in enumerate(unfiltered_2gram_words):\n",
    "    word1, word2 = term.split(' ')\n",
    "    if not (re.search(r'^[A-Za-z]', word1) and re.search(r'^[A-Za-z]', word2)):\n",
    "        continue\n",
    "    if not (re.search(r'[A-Za-z0-9]$', word1) and re.search(r'[A-Za-z0-9]$', word2)):\n",
    "        continue\n",
    "    if (re.search(r'[@#%&*()+=]', word1) or re.search(r'[@#%&*()+=]', word2)):\n",
    "        contiue\n",
    "    if not (len(word1) > 1 or len(word2) > 1):\n",
    "        continue\n",
    "    if ((word1.lower() in stemmed_stop_words) or (word2.lower() in stemmed_stop_words)):\n",
    "        continue\n",
    "    if ((word1.lower() in remove_words) or (word2.lower() in remove_words)):\n",
    "        continue\n",
    "        \n",
    "    filtered_2gram_words.append(term)\n",
    "    filtered_2gram_words_indices.append(i)\n",
    "\n",
    "print('Before filter: %d terms' % len(unfiltered_2gram_words))\n",
    "print('After filter: %d terms' % len(filtered_2gram_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 s, sys: 256 ms, total: 24.6 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = []\n",
    "for i in range(len(train['Text'])):\n",
    "    corpus.append(train.iloc[i]['Text'])\n",
    "\n",
    "# Vectorizer with the filtered terms\n",
    "ngram_vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2), vocabulary=filtered_2gram_words)\n",
    "vectorized_doc = ngram_vectorizer2.fit_transform(corpus)\n",
    "X = vectorized_doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.623\n",
      "Log Loss 1.729\n",
      "CPU times: user 7.44 s, sys: 57.9 ms, total: 7.5 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = np.array(class_train.Class).astype(int).ravel()\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=7, random_state=33)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=345)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, rfc.predict(X_test))\n",
    "lloss = log_loss(y_test, rfc.predict_proba(X_test), labels=list(range(1, 10)))\n",
    "\n",
    "print('Accuracy %.3f' % accuracy)\n",
    "print('Log Loss %.3f' % lloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
